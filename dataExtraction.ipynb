{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import dask.bag as dk\n",
    "import mysql.connector as db\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection\n",
    "cnx = db.connect(\n",
    "    host=\"localhost\",\n",
    "    port=3306,\n",
    "    user=\"root\",\n",
    "    password=\"root\"\n",
    ")\n",
    "\n",
    "db_curr = cnx.cursor()\n",
    "\n",
    "# Create database if it doesn't exist\n",
    "db_curr.execute(\"CREATE DATABASE IF NOT EXISTS cricket_dataset_practice\")\n",
    "\n",
    "# Connect to the database\n",
    "cnx = db.connect(\n",
    "    host=\"localhost\",\n",
    "    port=3306,\n",
    "    user=\"root\",\n",
    "    password=\"root\",\n",
    "    database=\"cricket_dataset_practice\",\n",
    "    autocommit=True\n",
    ")\n",
    "\n",
    "db_curr = cnx.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('match_id', 'int', 'NO', 'PRI', None, ''), ('city', 'varchar(255)', 'YES', '', None, ''), ('gender', 'varchar(255)', 'YES', '', None, ''), ('match_type', 'varchar(255)', 'YES', '', None, ''), ('match_type_number', 'int', 'YES', '', None, ''), ('overs', 'int', 'YES', '', None, ''), ('season', 'varchar(10)', 'YES', '', None, ''), ('team_type', 'varchar(255)', 'YES', '', None, ''), ('venue', 'varchar(255)', 'YES', '', None, ''), ('team1', 'varchar(255)', 'YES', '', None, ''), ('team2', 'varchar(255)', 'YES', '', None, ''), ('toss_winner', 'varchar(255)', 'YES', '', None, ''), ('toss_decision', 'varchar(255)', 'YES', '', None, ''), ('winner', 'varchar(255)', 'YES', '', None, ''), ('outcome_type', 'varchar(255)', 'YES', '', None, ''), ('outcome_value', 'varchar(255)', 'YES', '', None, ''), ('player_of_match', 'varchar(255)', 'YES', 'MUL', None, ''), ('balls_per_over', 'int', 'YES', '', None, '')]\n"
     ]
    }
   ],
   "source": [
    "#create match_details table\n",
    "db_curr.execute(\"\"\"CREATE TABLE match_details (\n",
    "    match_id INT PRIMARY KEY,\n",
    "    city VARCHAR(255),\n",
    "    gender VARCHAR(255),\n",
    "    match_type VARCHAR(255),\n",
    "    match_type_number INT,\n",
    "    overs INT,\n",
    "    season VARCHAR(10),\n",
    "    team_type VARCHAR(255),\n",
    "    venue VARCHAR(255),\n",
    "    team1 VARCHAR(255),\n",
    "    team2 VARCHAR(255),\n",
    "    toss_winner VARCHAR(255),\n",
    "    toss_decision VARCHAR(255),\n",
    "    winner VARCHAR(255),\n",
    "    outcome_type VARCHAR(255),\n",
    "    outcome_value VARCHAR(255),\n",
    "    player_of_match VARCHAR(255),\n",
    "    balls_per_over INT,\n",
    "    FOREIGN KEY (player_of_match) REFERENCES registry(person_id)            \n",
    ");\"\"\")\n",
    "db_curr.execute(\"\"\"DESC match_details;\"\"\")\n",
    "print(db_curr.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'int', 'NO', 'PRI', None, 'auto_increment'), ('match_id', 'int', 'YES', 'MUL', None, ''), ('person_id', 'varchar(255)', 'YES', 'MUL', None, ''), ('official_type', 'varchar(255)', 'YES', '', None, '')]\n"
     ]
    }
   ],
   "source": [
    "db_curr.execute(\"\"\"CREATE TABLE officials (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    match_id INT,\n",
    "    person_id VARCHAR(255),\n",
    "    official_type VARCHAR(255),\n",
    "    FOREIGN KEY (match_id) REFERENCES match_details(match_id),\n",
    "    FOREIGN KEY (person_id) REFERENCES registry(person_id)\n",
    ");\"\"\")\n",
    "db_curr.execute(\"\"\"DESC officials;\"\"\")\n",
    "print(db_curr.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('person_id', 'varchar(255)', 'NO', 'PRI', None, ''), ('person_name', 'varchar(255)', 'YES', '', None, '')]\n"
     ]
    }
   ],
   "source": [
    "db_curr.execute(\"\"\"CREATE TABLE registry (\n",
    "    person_id VARCHAR(255) PRIMARY KEY,\n",
    "    person_name VARCHAR(255)\n",
    ");\"\"\")\n",
    "db_curr.execute(\"\"\"DESC registry;\"\"\")\n",
    "print(db_curr.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'int', 'NO', 'PRI', None, 'auto_increment'), ('match_id', 'int', 'YES', 'MUL', None, ''), ('person_id', 'varchar(255)', 'YES', 'MUL', None, ''), ('team_name', 'varchar(255)', 'YES', '', None, '')]\n"
     ]
    }
   ],
   "source": [
    "db_curr.execute(\"\"\"CREATE TABLE players (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    match_id INT,\n",
    "    person_id VARCHAR(255),\n",
    "    team_name VARCHAR(255),\n",
    "    FOREIGN KEY (match_id) REFERENCES match_details(match_id),\n",
    "    FOREIGN KEY (person_id) REFERENCES registry(person_id)\n",
    ");\"\"\")\n",
    "db_curr.execute(\"\"\"DESC players;\"\"\")\n",
    "print(db_curr.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'int', 'NO', 'PRI', None, 'auto_increment'), ('match_id', 'int', 'YES', 'MUL', None, ''), ('innings', 'int', 'YES', '', None, ''), ('team', 'varchar(255)', 'YES', '', None, ''), ('overs', 'int', 'YES', '', None, ''), ('balls', 'int', 'YES', '', None, ''), ('batter', 'varchar(255)', 'YES', 'MUL', None, ''), ('bowler', 'varchar(255)', 'YES', 'MUL', None, ''), ('non_striker', 'varchar(255)', 'YES', 'MUL', None, ''), ('runs_batter', 'int', 'YES', '', None, ''), ('runs_extras', 'int', 'YES', '', None, ''), ('runs_total', 'int', 'YES', '', None, ''), ('powerplayed', 'varchar(255)', 'YES', '', None, ''), ('powerplayed_type', 'varchar(255)', 'YES', '', None, ''), ('player_out', 'varchar(255)', 'YES', 'MUL', None, ''), ('dismissal_kind', 'varchar(255)', 'YES', '', None, ''), ('fielders_involved', 'varchar(255)', 'YES', 'MUL', None, '')]\n"
     ]
    }
   ],
   "source": [
    "db_curr.execute(\"\"\"CREATE TABLE deliveries (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    match_id INT,  \n",
    "    innings INT,\n",
    "    team VARCHAR(255),\n",
    "    overs INT,\n",
    "    balls INT,\n",
    "    batter VARCHAR(255),\n",
    "    bowler VARCHAR(255),\n",
    "    non_striker VARCHAR(255),\n",
    "    runs_batter INT,\n",
    "    runs_extras INT,\n",
    "    runs_total INT,\n",
    "    powerplayed VARCHAR(255),\n",
    "    powerplayed_type VARCHAR(255),\n",
    "    player_out VARCHAR(255),\n",
    "    dismissal_kind VARCHAR(255),\n",
    "    fielders_involved VARCHAR(255),\n",
    "    FOREIGN KEY (match_id) REFERENCES match_details(match_id),  -- Foreign key to match_details\n",
    "    FOREIGN KEY (batter) REFERENCES registry(person_id),       -- Foreign key to registry for batter\n",
    "    FOREIGN KEY (bowler) REFERENCES registry(person_id),       -- Foreign key to registry for bowler\n",
    "    FOREIGN KEY (non_striker) REFERENCES registry(person_id),  -- Foreign key to registry for non-striker\n",
    "    FOREIGN KEY (player_out) REFERENCES registry(person_id),    -- Foreign key to registry for player_out\n",
    "    FOREIGN KEY (fielders_involved) REFERENCES registry(person_id)    -- Foreign key to registry for fielders_involved\n",
    ");\"\"\")\n",
    "db_curr.execute(\"\"\"DESC deliveries;\"\"\")\n",
    "print(db_curr.fetchall())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dataframe_DB(df_match_details,df_officials,df_registry,df_players,df_deliveries):\n",
    "    # Create a new MySQL connection\n",
    "    connection = db.connect(\n",
    "        host='localhost',\n",
    "        database='your_database',\n",
    "        user='your_user',\n",
    "        password='your_password'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        cursor = connection.cursor()\n",
    "        # Process data and insert into database\n",
    "        # ...\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {matchID}: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_match_data(matchID, data):\n",
    "    d = data['info']\n",
    "    registry = d['registry']['people']\n",
    "    \n",
    "    # Create a dictionary to store the match details\n",
    "    match = {\n",
    "        \"match_id\": matchID, \n",
    "        \"city\": d['city'], \n",
    "        \"gender\": d['gender'],\n",
    "        \"match_type\": d[\"match_type\"], \n",
    "        \"match_type_number\": d[\"match_type_number\"],\n",
    "        \"overs\": d[\"overs\"], \n",
    "        \"season\": d[\"season\"], \n",
    "        \"team_type\": d[\"team_type\"],\n",
    "        \"venue\": d[\"venue\"], \n",
    "        \"team1\": d[\"teams\"][0], \n",
    "        \"team2\": d[\"teams\"][1],\n",
    "        \"toss_winner\": d[\"toss\"][\"winner\"], \n",
    "        \"toss_decision\": d[\"toss\"][\"decision\"],\n",
    "        \"winner\": d[\"outcome\"].get(\"winner\"),\n",
    "        \"outcome_type\": next(iter(d[\"outcome\"].get(\"by\", {})), d[\"outcome\"].get(\"result\")),\n",
    "        \"outcome_value\": d[\"outcome\"].get(\"by\"),\n",
    "        \"player_of_match\": ', '.join(registry.get(p) for p in d.get('player_of_match', [])),\n",
    "        \"balls_per_over\": d[\"balls_per_over\"]\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame for the match details\n",
    "    df_match_details = pd.DataFrame([match])\n",
    "    df_match_details.to_sql('match_details', cnx, if_exists='append', index=False)\n",
    "    \n",
    "    \n",
    "    # Create DataFrames for officials, registry, and players\n",
    "    df_officials = pd.DataFrame([\n",
    "        {\"match_id\": matchID, \"person_id\": registry[person], \"official_type\": off}\n",
    "        for off in d['officials'] for person in d['officials'][off]\n",
    "    ])\n",
    "    \n",
    "    df_registry = pd.DataFrame({\n",
    "        \"person_id\": list(registry.values()),\n",
    "        \"person_name\": list(registry.keys())\n",
    "    }).drop_duplicates()\n",
    "    \n",
    "    df_players = pd.DataFrame([\n",
    "        {\"match_id\": matchID, \"person_id\": registry[player], \"team_name\": team}\n",
    "        for team, players in d['players'].items() for player in players\n",
    "    ])\n",
    "\n",
    "    df_officials.to_sql('officials', cnx, if_exists='append', index=False)\n",
    "    df_registry.to_sql('registry', cnx, if_exists='append', index=False)\n",
    "    df_players.to_sql('players', cnx, if_exists='append', index=False)\n",
    "    \n",
    "\n",
    "    # Create a list to store the deliveries\n",
    "    deliveries = []\n",
    "    \n",
    "    # Iterate over the innings\n",
    "    for inning_count, inning in enumerate(data['innings'], start=1):\n",
    "        team = inning['team']\n",
    "        powerplays = {pp['type']: (int(pp['from']), int(pp['to'])) for pp in inning.get('powerplays', [])}\n",
    "        miscounted = {int(k): v['balls'] for k, v in inning.get('miscounted_overs', {}).items()}\n",
    "        \n",
    "        # Iterate over the overs\n",
    "        for over in inning['overs']:\n",
    "            over_num = over['over']\n",
    "            balls_limit = miscounted.get(over['over'], 6)\n",
    "            \n",
    "            # Iterate over the deliveries\n",
    "            for ball_count, delivery in enumerate(over['deliveries'], start=1):\n",
    "                if ball_count > balls_limit: continue\n",
    "                \n",
    "                # Get the powerplay type\n",
    "                powerplay_type = next((t for t, (s, e) in powerplays.items() if s <= over_num <= e), 'none')\n",
    "                \n",
    "                # Get the wickets\n",
    "                wickets = delivery.get('wickets', [{}])[0]\n",
    "                \n",
    "                # Append the delivery to the list\n",
    "                deliveries.append({\n",
    "                    'innings': inning_count, \n",
    "                    'team': team, \n",
    "                    'overs': over_num, \n",
    "                    'balls': ball_count,\n",
    "                    'batter': registry.get(delivery['batter']), \n",
    "                    'bowler': registry.get(delivery['bowler']),\n",
    "                    'non_striker': registry.get(delivery['non_striker']),\n",
    "                    'runs_batter': delivery['runs']['batter'], \n",
    "                    'runs_extras': delivery['runs']['extras'],\n",
    "                    'runs_total': delivery['runs']['total'],\n",
    "                    'powerplayed': 'yes' if powerplay_type != 'none' else 'no',\n",
    "                    'powerplayed_type': powerplay_type,\n",
    "                    'player_out': registry.get(wickets.get('player_out')),\n",
    "                    'dismissal_kind': wickets.get('kind'),\n",
    "                    'fielders_involved': ', '.join(registry.get(f['name']) for f in wickets.get('fielders', [])) if wickets.get('fielders') else None\n",
    "                })\n",
    "    \n",
    "    # Create a DataFrame for the deliveries\n",
    "    df_deliveries = pd.DataFrame(deliveries)\n",
    "    df_deliveries.to_sql('deliveries', cnx, if_exists='append', index=False)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            match_ID = os.path.basename(filepath).rsplit('.', 1)[0]  # Extract match_ID\n",
    "            # Process data and return a dictionary or suitable format\n",
    "            extract_match_data(match_ID,data) # Call your function to process the data\n",
    "            return match_ID\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {filepath}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle '_mysql_connector.MySQL' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m matches_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatches\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m formats \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124modis_json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt20s_json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtests_json\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 33\u001b[0m \u001b[43mprocess_json_files_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatches_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# assign all the dataframes.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[22], line 25\u001b[0m, in \u001b[0;36mprocess_json_files_parallel\u001b[1;34m(matches_path, formats)\u001b[0m\n\u001b[0;32m     23\u001b[0m     bags\u001b[38;5;241m.\u001b[39mappend(dk\u001b[38;5;241m.\u001b[39mfrom_sequence(filepaths))\n\u001b[0;32m     24\u001b[0m combined_bag \u001b[38;5;241m=\u001b[39m dk\u001b[38;5;241m.\u001b[39mconcat(bags)\n\u001b[1;32m---> 25\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcombined_bag\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_json_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results))\n",
      "File \u001b[1;32me:\\AI engineer\\Guvi\\Capstone Projects\\Project2\\venv\\Lib\\site-packages\\dask\\base.py:374\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 374\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32me:\\AI engineer\\Guvi\\Capstone Projects\\Project2\\venv\\Lib\\site-packages\\dask\\base.py:662\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    659\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 662\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32me:\\AI engineer\\Guvi\\Capstone Projects\\Project2\\venv\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1537\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m   1535\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m   1536\u001b[0m     cp \u001b[38;5;241m=\u001b[39m Pickler(file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback)\n\u001b[1;32m-> 1537\u001b[0m     \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32me:\\AI engineer\\Guvi\\Capstone Projects\\Project2\\venv\\Lib\\site-packages\\cloudpickle\\cloudpickle.py:1303\u001b[0m, in \u001b[0;36mPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m   1301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[0;32m   1302\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1304\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle '_mysql_connector.MySQL' object"
     ]
    }
   ],
   "source": [
    "@delayed\n",
    "def get_json_files(format_path):\n",
    "    files = []\n",
    "    if os.path.isdir(format_path):\n",
    "        for file in os.listdir(format_path):\n",
    "            if file.endswith(\".json\"):\n",
    "                filepath = os.path.join(format_path, file)\n",
    "                files.append(filepath)\n",
    "    return files\n",
    "\n",
    "def process_json_files_parallel(matches_path, formats):\n",
    "    all_filepaths = []\n",
    "\n",
    "    for format_name in formats:\n",
    "        format_path = os.path.join(matches_path, format_name)\n",
    "        files = get_json_files(format_path)\n",
    "        all_filepaths.append(files)\n",
    "\n",
    "    all_filepaths = dk.compute(*all_filepaths)\n",
    "\n",
    "    bags = []\n",
    "    for filepaths in all_filepaths:\n",
    "        bags.append(dk.from_sequence(filepaths))\n",
    "    combined_bag = dk.concat(bags)\n",
    "    results = combined_bag.map(process_json_file).compute()\n",
    "    print(len(results))\n",
    "\n",
    "\n",
    "\n",
    "# Example usage (same as before):\n",
    "matches_path = os.path.join(\"matches\")\n",
    "formats = [\"odis_json\", \"t20s_json\", \"tests_json\"]\n",
    "process_json_files_parallel(matches_path, formats) # assign all the dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match_details, registry, officials, players, deliveries\n",
    "db_curr.execute(\"\"\"TRUNCATE TABLE ;\"\"\")\n",
    "print(db_curr.fetchall())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
